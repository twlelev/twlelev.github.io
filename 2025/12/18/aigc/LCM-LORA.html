<hr>
<h2 id="title-LCM-LORAtop-falsecover-falsetoc-truemathjax-truedate-2025-12-19-15-04-10password-summary-tags-AIGCcategories-AIGC"><a href="#title-LCM-LORAtop-falsecover-falsetoc-truemathjax-truedate-2025-12-19-15-04-10password-summary-tags-AIGCcategories-AIGC" class="headerlink" title="title: LCM LORAtop: falsecover: falsetoc: truemathjax: truedate: 2025-12-19 15:04:10password:summary:tags:  - AIGCcategories:  - AIGC"></a>title: LCM LORA<br>top: false<br>cover: false<br>toc: true<br>mathjax: true<br>date: 2025-12-19 15:04:10<br>password:<br>summary:<br>tags:<br>  - AIGC<br>categories:<br>  - AIGC</h2><h1 id="技术深度解析：LCM-LoRA-原理与-Scheduler-源码实现"><a href="#技术深度解析：LCM-LoRA-原理与-Scheduler-源码实现" class="headerlink" title="技术深度解析：LCM-LoRA 原理与 Scheduler 源码实现"></a>技术深度解析：LCM-LoRA 原理与 Scheduler 源码实现</h1><h2 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1. 背景介绍"></a>1. 背景介绍</h2><p>在扩散模型（Diffusion Models）的推理过程中，传统的采样方法（如 DDIM, Euler）通常需要 20-50 步迭代。<strong>一致性模型（Consistency Models, CM）</strong> 的出现旨在打破这一限制，通过学习一个“一致性映射”，模型可以直接从噪声状态 $x_t$ 预测到 $x_0$。<br><strong>本质：</strong> 不是在求解 ODE，而是在直接预测 ODE 轨迹的<strong>不动点</strong>。</p>
<p><strong>LCM-LoRA</strong> 则是将 LCM 的蒸馏技术与 LoRA（低秩自适应）结合，使得原本庞大的采样步数可以压缩至 2-8 步，且可以像普通 LoRA 一样灵活插拔，极大地降低了推理成本。</p>
<hr>
<h2 id="2-核心数学理论"><a href="#2-核心数学理论" class="headerlink" title="2. 核心数学理论"></a>2. 核心数学理论</h2><h3 id="2-1-概率流-ODE-PF-ODE"><a href="#2-1-概率流-ODE-PF-ODE" class="headerlink" title="2.1 概率流 ODE (PF-ODE)"></a>2.1 概率流 ODE (PF-ODE)</h3><p>扩散模型的前向过程可以映射为一个概率流常微分方程（Probability Flow ODE）。传统的采样器本质上是在数值求解这个 ODE。</p>
<h3 id="2-2-一致性映射（Consistency-Mapping）"><a href="#2-2-一致性映射（Consistency-Mapping）" class="headerlink" title="2.2 一致性映射（Consistency Mapping）"></a>2.2 一致性映射（Consistency Mapping）</h3><p>一致性模型旨在学习一个函数 $f(x_t, t)$，其满足： $$f(x_t, t) &#x3D; f(x_{t’}, t’) &#x3D; x_0$$ 这意味着对于同一条 PF-ODE 轨迹上的任何点，无论时间步 $t$ 是多少，经过该函数映射后都应该指向同一个终点 $x_0$。</p>
<h3 id="2-3-LCM-蒸馏公式"><a href="#2-3-LCM-蒸馏公式" class="headerlink" title="2.3 LCM 蒸馏公式"></a>2.3 LCM 蒸馏公式</h3><p>LCM 在训练时使用预训练模型作为指导，通过以下边界条件定义映射： $$\hat{x}<em>0 &#x3D; f_\theta(x_t, t) &#x3D; c</em>{skip}(t)x_t + c_{out}(t)F_\theta(x_t, t)$$ 其中：</p>
<ul>
<li>$c_{skip}(t)$ 和 $c_{out}(t)$ 是缩放系数，确保在 $t&#x3D;0$ 时满足边界条件 $f(x_0, 0) &#x3D; x_0$。</li>
<li>$F_\theta(x_t, t)$ 是神经网络预测的内容（通常是基于噪声预测）。</li>
</ul>
<h2 id="3-Scheduler-源码深度分析"><a href="#3-Scheduler-源码深度分析" class="headerlink" title="3. Scheduler 源码深度分析"></a>3. Scheduler 源码深度分析</h2><p>结合 <code>diffusers</code> 库中的 <code>LCMScheduler</code>，我们可以看到理论是如何转化为代码的。</p>
<h3 id="核心步骤-1：预测并还原-x-0"><a href="#核心步骤-1：预测并还原-x-0" class="headerlink" title="核心步骤 1：预测并还原 $x_0$"></a>核心步骤 1：预测并还原 $x_0$</h3><p>这是所有去噪逻辑的起点。调度器根据 UNet 预测的噪声 $\epsilon_\theta$（即代码中的 model_output）反推 $x_0$：</p>
<pre><code class="python"># 理论公式：x_0 = (x_t - sqrt(1 - alpha_t) * epsilon) / sqrt(alpha_t)
pred_original_sample = (sample - (1 - alpha_prod_t)**0.5 * model_output) / alpha_prod_t**0.5
</code></pre>
<h3 id="核心步骤-2：应用一致性比例进行跳跃"><a href="#核心步骤-2：应用一致性比例进行跳跃" class="headerlink" title="核心步骤 2：应用一致性比例进行跳跃"></a>核心步骤 2：应用一致性比例进行跳跃</h3><p>传统调度器会根据导数走一小步，而 LCM 调度器通过 <code>c_skip</code> 和 <code>c_out</code> 执行“跨步”：</p>
<pre><code class="python"># c_skip 和 c_out 由当前时间步 t 决定
c_skip, c_out = self.get_scalings_for_boundary_condition(timestep)

# 直接计算下一状态 (LCM 模型的一致性步进)
# prev_sample = x_{t-1}
prev_sample = c_skip * sample + c_out * pred_original_sample
</code></pre>
<h3 id="核心步骤-3：多步随机性补偿"><a href="#核心步骤-3：多步随机性补偿" class="headerlink" title="核心步骤 3：多步随机性补偿"></a>核心步骤 3：多步随机性补偿</h3><p>为了增强生成的纹理细节，如果是运行多步（Multi-step LCM），会在最后注入微量的高斯噪声：</p>
<pre><code class="python">if self.num_inference_steps &gt; 1 and timestep &gt; 0:
    noise = randn_tensor(model_output.shape, generator=generator)
    prev_sample = prev_sample + noise * self.sigmas[timestep]
</code></pre>
